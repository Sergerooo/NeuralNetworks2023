{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите простую задачу безусловной оптимизации (минимизации) в двумерном пространстве:  \n",
    "$$f(\\boldsymbol x) = -8x_1 - 16x_2 + x_1^2 + 4x_2^2$$\n",
    "используя два метода:\n",
    " - аналитически (функция квадратичная, выпуклая)\n",
    " - методом градиентного спуска, используя один из методов оптимизации torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:26:53.044079157Z",
     "start_time": "2023-10-06T08:26:51.319344496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 tensor([4.], requires_grad=True), x2 tensor([2.4000], requires_grad=True), loss tensor([-16.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([16.])\n",
      "iteration 1, x1 tensor([4.], requires_grad=True), x2 tensor([2.0800], requires_grad=True), loss tensor([-31.3600], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([3.2000])\n",
      "iteration 2, x1 tensor([4.], requires_grad=True), x2 tensor([2.0160], requires_grad=True), loss tensor([-31.9744], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.6400])\n",
      "iteration 3, x1 tensor([4.], requires_grad=True), x2 tensor([2.0032], requires_grad=True), loss tensor([-31.9990], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.1280])\n",
      "iteration 4, x1 tensor([4.], requires_grad=True), x2 tensor([2.0006], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0256])\n",
      "iteration 5, x1 tensor([4.], requires_grad=True), x2 tensor([2.0001], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0051])\n",
      "iteration 6, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0010])\n",
      "iteration 7, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0002])\n",
      "iteration 8, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([4.0054e-05])\n",
      "iteration 9, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([7.6294e-06])\n",
      "iteration 10, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([1.9073e-06])\n",
      "iteration 11, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 12, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 13, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 14, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 15, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 16, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 17, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 18, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 19, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 20, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 21, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 22, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 23, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 24, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 25, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 26, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 27, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 28, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 29, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 30, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 31, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 32, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 33, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 34, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 35, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 36, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 37, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 38, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 39, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 40, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 41, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 42, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 43, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 44, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 45, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 46, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 47, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 48, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 49, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 50, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 51, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 52, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 53, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 54, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 55, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 56, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 57, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 58, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 59, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 60, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 61, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 62, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 63, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 64, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 65, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 66, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 67, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 68, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 69, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 70, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 71, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 72, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 73, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 74, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 75, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 76, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 77, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 78, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 79, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 80, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 81, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 82, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 83, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 84, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 85, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 86, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 87, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 88, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 89, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 90, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 91, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 92, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 93, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 94, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 95, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 96, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 97, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 98, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 99, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(x1, x2):\n",
    "    return -8*x1 - 16*x2 + x1**2 + 4*x2**2\n",
    "\n",
    "x1 = torch.tensor([4], dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([4], dtype=torch.float32, requires_grad=True)\n",
    "optimizer = torch.optim.SGD([x1, x2], lr=0.1)\n",
    "\n",
    "for i in range(100):\n",
    "    y = f(x1, x2)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    print(f'iteration {i}, x1 {x1}, x2 {x2}, loss {y}, grad {x1.grad}, grad {x2.grad}')\n",
    "    #     with torch.no_grad():\n",
    "    #         x -= 0.1 * x.grad\n",
    "    #x.grad.zero_()\n",
    "    #print(f'grad {x.grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:35:44.388152044Z",
     "start_time": "2023-10-06T08:35:44.097189501Z"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Альтернатива:\n",
    "```python\n",
    "def f(x1, x2):\n",
    "    return -8*x1 - 16*x2 + x1**2 + 4*x2**2\n",
    "\n",
    "x1 = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "for i in range(100):\n",
    "    y = f(x1, x2)\n",
    "    y.backward()\n",
    "    print(f'iteration {i}, x1 {x1}, x2 {x2}, loss {y}, grad {x1.grad}, grad {x2.grad}')\n",
    "    with torch.no_grad():\n",
    "        x1 -= 0.01 * x1.grad\n",
    "        x2 -= 0.01 * x2.grad\n",
    "    x1.grad.zero_()\n",
    "    x2.grad.zero_()\n",
    "    #print(f'grad {x.grad}')\n",
    "```"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
