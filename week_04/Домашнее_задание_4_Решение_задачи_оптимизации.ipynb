{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Решите простую задачу безусловной оптимизации (минимизации) в двумерном пространстве:  \n",
    "$$f(\\boldsymbol x) = -8x_1 - 16x_2 + x_1^2 + 4x_2^2$$\n",
    "используя два метода:\n",
    " - аналитически (функция квадратичная, выпуклая)\n",
    " - методом градиентного спуска, используя один из методов оптимизации torch.optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:26:53.044079157Z",
     "start_time": "2023-10-06T08:26:51.319344496Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 4.0, x2 4.0, loss -16.0, grad 0.0, grad 16.0\n",
      "iteration 1, x1 4.0, x2 3.8399999141693115, loss -18.457603454589844, grad 0.0, grad 14.719999313354492\n",
      "iteration 2, x1 4.0, x2 3.6928000450134277, loss -20.53771209716797, grad 0.0, grad 13.542400360107422\n",
      "iteration 3, x1 4.0, x2 3.5573761463165283, loss -22.29831314086914, grad 0.0, grad 12.459009170532227\n",
      "iteration 4, x1 4.0, x2 3.432785987854004, loss -23.788497924804688, grad 0.0, grad 11.462287902832031\n",
      "iteration 5, x1 4.0, x2 3.3181631565093994, loss -25.04977798461914, grad 0.0, grad 10.545305252075195\n",
      "iteration 6, x1 4.0, x2 3.21271014213562, loss -26.117340087890625, grad 0.0, grad 9.701681137084961\n",
      "iteration 7, x1 4.0, x2 3.1156933307647705, loss -27.02090835571289, grad 0.0, grad 8.925546646118164\n",
      "iteration 8, x1 4.0, x2 3.026437759399414, loss -27.785701751708984, grad 0.0, grad 8.211502075195312\n",
      "iteration 9, x1 4.0, x2 2.9443228244781494, loss -28.433013916015625, grad 0.0, grad 7.554582595825195\n",
      "iteration 10, x1 4.0, x2 2.86877703666687, loss -28.980911254882812, grad 0.0, grad 6.950216293334961\n",
      "iteration 11, x1 4.0, x2 2.7992749214172363, loss -29.444639205932617, grad 0.0, grad 6.394199371337891\n",
      "iteration 12, x1 4.0, x2 2.73533296585083, loss -29.837141036987305, grad 0.0, grad 5.882663726806641\n",
      "iteration 13, x1 4.0, x2 2.676506280899048, loss -30.169353485107422, grad 0.0, grad 5.412050247192383\n",
      "iteration 14, x1 4.0, x2 2.6223857402801514, loss -30.45054817199707, grad 0.0, grad 4.979085922241211\n",
      "iteration 15, x1 4.0, x2 2.5725948810577393, loss -30.688535690307617, grad 0.0, grad 4.580759048461914\n",
      "iteration 16, x1 4.0, x2 2.526787281036377, loss -30.88998031616211, grad 0.0, grad 4.214298248291016\n",
      "iteration 17, x1 4.0, x2 2.4846444129943848, loss -31.06048011779785, grad 0.0, grad 3.877155303955078\n",
      "iteration 18, x1 4.0, x2 2.4458727836608887, loss -31.204790115356445, grad 0.0, grad 3.5669822692871094\n",
      "iteration 19, x1 4.0, x2 2.410202980041504, loss -31.326934814453125, grad 0.0, grad 3.2816238403320312\n",
      "iteration 20, x1 4.0, x2 2.3773868083953857, loss -31.430320739746094, grad 0.0, grad 3.019094467163086\n",
      "iteration 21, x1 4.0, x2 2.347195863723755, loss -31.5178165435791, grad 0.0, grad 2.777566909790039\n",
      "iteration 22, x1 4.0, x2 2.319420099258423, loss -31.59187889099121, grad 0.0, grad 2.555360794067383\n",
      "iteration 23, x1 4.0, x2 2.2938663959503174, loss -31.654565811157227, grad 0.0, grad 2.350931167602539\n",
      "iteration 24, x1 4.0, x2 2.270357131958008, loss -31.70762825012207, grad 0.0, grad 2.1628570556640625\n",
      "iteration 25, x1 4.0, x2 2.2487285137176514, loss -31.752540588378906, grad 0.0, grad 1.989828109741211\n",
      "iteration 26, x1 4.0, x2 2.228830337524414, loss -31.790546417236328, grad 0.0, grad 1.8306427001953125\n",
      "iteration 27, x1 4.0, x2 2.210523843765259, loss -31.822715759277344, grad 0.0, grad 1.6841907501220703\n",
      "iteration 28, x1 4.0, x2 2.1936819553375244, loss -31.849945068359375, grad 0.0, grad 1.5494556427001953\n",
      "iteration 29, x1 4.0, x2 2.178187370300293, loss -31.872997283935547, grad 0.0, grad 1.4254989624023438\n",
      "iteration 30, x1 4.0, x2 2.1639323234558105, loss -31.892505645751953, grad 0.0, grad 1.3114585876464844\n",
      "iteration 31, x1 4.0, x2 2.150817632675171, loss -31.909019470214844, grad 0.0, grad 1.2065410614013672\n",
      "iteration 32, x1 4.0, x2 2.1387522220611572, loss -31.92298698425293, grad 0.0, grad 1.1100177764892578\n",
      "iteration 33, x1 4.0, x2 2.1276519298553467, loss -31.934823989868164, grad 0.0, grad 1.0212154388427734\n",
      "iteration 34, x1 4.0, x2 2.1174397468566895, loss -31.94483184814453, grad 0.0, grad 0.9395179748535156\n",
      "iteration 35, x1 4.0, x2 2.1080446243286133, loss -31.953306198120117, grad 0.0, grad 0.8643569946289062\n",
      "iteration 36, x1 4.0, x2 2.0994009971618652, loss -31.960477828979492, grad 0.0, grad 0.7952079772949219\n",
      "iteration 37, x1 4.0, x2 2.091449022293091, loss -31.96654510498047, grad 0.0, grad 0.7315921783447266\n",
      "iteration 38, x1 4.0, x2 2.0841331481933594, loss -31.97168731689453, grad 0.0, grad 0.673065185546875\n",
      "iteration 39, x1 4.0, x2 2.0774025917053223, loss -31.976036071777344, grad 0.0, grad 0.6192207336425781\n",
      "iteration 40, x1 4.0, x2 2.0712103843688965, loss -31.979717254638672, grad 0.0, grad 0.5696830749511719\n",
      "iteration 41, x1 4.0, x2 2.0655136108398438, loss -31.982831954956055, grad 0.0, grad 0.52410888671875\n",
      "iteration 42, x1 4.0, x2 2.060272455215454, loss -31.98546600341797, grad 0.0, grad 0.4821796417236328\n",
      "iteration 43, x1 4.0, x2 2.055450677871704, loss -31.98769760131836, grad 0.0, grad 0.4436054229736328\n",
      "iteration 44, x1 4.0, x2 2.0510146617889404, loss -31.989593505859375, grad 0.0, grad 0.40811729431152344\n",
      "iteration 45, x1 4.0, x2 2.04693341255188, loss -31.991186141967773, grad 0.0, grad 0.37546730041503906\n",
      "iteration 46, x1 4.0, x2 2.0431787967681885, loss -31.992538452148438, grad 0.0, grad 0.3454303741455078\n",
      "iteration 47, x1 4.0, x2 2.039724588394165, loss -31.993684768676758, grad 0.0, grad 0.3177967071533203\n",
      "iteration 48, x1 4.0, x2 2.0365467071533203, loss -31.994657516479492, grad 0.0, grad 0.2923736572265625\n",
      "iteration 49, x1 4.0, x2 2.033622980117798, loss -31.995473861694336, grad 0.0, grad 0.2689838409423828\n",
      "iteration 50, x1 4.0, x2 2.030933141708374, loss -31.99617576599121, grad 0.0, grad 0.2474651336669922\n",
      "iteration 51, x1 4.0, x2 2.028458595275879, loss -31.996761322021484, grad 0.0, grad 0.22766876220703125\n",
      "iteration 52, x1 4.0, x2 2.026181936264038, loss -31.99726104736328, grad 0.0, grad 0.2094554901123047\n",
      "iteration 53, x1 4.0, x2 2.024087429046631, loss -31.997678756713867, grad 0.0, grad 0.19269943237304688\n",
      "iteration 54, x1 4.0, x2 2.022160530090332, loss -31.998035430908203, grad 0.0, grad 0.17728424072265625\n",
      "iteration 55, x1 4.0, x2 2.020387649536133, loss -31.998336791992188, grad 0.0, grad 0.1631011962890625\n",
      "iteration 56, x1 4.0, x2 2.018756628036499, loss -31.99859619140625, grad 0.0, grad 0.1500530242919922\n",
      "iteration 57, x1 4.0, x2 2.017256021499634, loss -31.99880599975586, grad 0.0, grad 0.1380481719970703\n",
      "iteration 58, x1 4.0, x2 2.0158755779266357, loss -31.998994827270508, grad 0.0, grad 0.12700462341308594\n",
      "iteration 59, x1 4.0, x2 2.0146055221557617, loss -31.999147415161133, grad 0.0, grad 0.11684417724609375\n",
      "iteration 60, x1 4.0, x2 2.013437032699585, loss -31.99928092956543, grad 0.0, grad 0.10749626159667969\n",
      "iteration 61, x1 4.0, x2 2.012362003326416, loss -31.9993896484375, grad 0.0, grad 0.09889602661132812\n",
      "iteration 62, x1 4.0, x2 2.0113730430603027, loss -31.999483108520508, grad 0.0, grad 0.09098434448242188\n",
      "iteration 63, x1 4.0, x2 2.010463237762451, loss -31.999561309814453, grad 0.0, grad 0.08370590209960938\n",
      "iteration 64, x1 4.0, x2 2.0096261501312256, loss -31.9996337890625, grad 0.0, grad 0.07700920104980469\n",
      "iteration 65, x1 4.0, x2 2.0088560581207275, loss -31.999683380126953, grad 0.0, grad 0.07084846496582031\n",
      "iteration 66, x1 4.0, x2 2.0081474781036377, loss -31.999731063842773, grad 0.0, grad 0.06517982482910156\n",
      "iteration 67, x1 4.0, x2 2.007495641708374, loss -31.999778747558594, grad 0.0, grad 0.05996513366699219\n",
      "iteration 68, x1 4.0, x2 2.0068960189819336, loss -31.99980926513672, grad 0.0, grad 0.05516815185546875\n",
      "iteration 69, x1 4.0, x2 2.0063443183898926, loss -31.999839782714844, grad 0.0, grad 0.050754547119140625\n",
      "iteration 70, x1 4.0, x2 2.0058367252349854, loss -31.999860763549805, grad 0.0, grad 0.04669380187988281\n",
      "iteration 71, x1 4.0, x2 2.0053699016571045, loss -31.999889373779297, grad 0.0, grad 0.04295921325683594\n",
      "iteration 72, x1 4.0, x2 2.0049402713775635, loss -31.99989891052246, grad 0.0, grad 0.03952217102050781\n",
      "iteration 73, x1 4.0, x2 2.004544973373413, loss -31.999921798706055, grad 0.0, grad 0.03635978698730469\n",
      "iteration 74, x1 4.0, x2 2.004181385040283, loss -31.999929428100586, grad 0.0, grad 0.033451080322265625\n",
      "iteration 75, x1 4.0, x2 2.0038468837738037, loss -31.99994468688965, grad 0.0, grad 0.030775070190429688\n",
      "iteration 76, x1 4.0, x2 2.0035390853881836, loss -31.999950408935547, grad 0.0, grad 0.02831268310546875\n",
      "iteration 77, x1 4.0, x2 2.003255844116211, loss -31.999958038330078, grad 0.0, grad 0.0260467529296875\n",
      "iteration 78, x1 4.0, x2 2.002995491027832, loss -31.999963760375977, grad 0.0, grad 0.02396392822265625\n",
      "iteration 79, x1 4.0, x2 2.002755880355835, loss -31.99997329711914, grad 0.0, grad 0.022047042846679688\n",
      "iteration 80, x1 4.0, x2 2.002535343170166, loss -31.999975204467773, grad 0.0, grad 0.020282745361328125\n",
      "iteration 81, x1 4.0, x2 2.0023324489593506, loss -31.999982833862305, grad 0.0, grad 0.018659591674804688\n",
      "iteration 82, x1 4.0, x2 2.002145767211914, loss -31.999980926513672, grad 0.0, grad 0.0171661376953125\n",
      "iteration 83, x1 4.0, x2 2.001974105834961, loss -31.999984741210938, grad 0.0, grad 0.0157928466796875\n",
      "iteration 84, x1 4.0, x2 2.0018162727355957, loss -31.99998664855957, grad 0.0, grad 0.014530181884765625\n",
      "iteration 85, x1 4.0, x2 2.001671075820923, loss -31.999984741210938, grad 0.0, grad 0.013368606567382812\n",
      "iteration 86, x1 4.0, x2 2.001537322998047, loss -31.999990463256836, grad 0.0, grad 0.012298583984375\n",
      "iteration 87, x1 4.0, x2 2.0014142990112305, loss -31.99999237060547, grad 0.0, grad 0.01131439208984375\n",
      "iteration 88, x1 4.0, x2 2.0013010501861572, loss -31.999988555908203, grad 0.0, grad 0.010408401489257812\n",
      "iteration 89, x1 4.0, x2 2.00119686126709, loss -31.9999942779541, grad 0.0, grad 0.00957489013671875\n",
      "iteration 90, x1 4.0, x2 2.001101016998291, loss -31.9999942779541, grad 0.0, grad 0.008808135986328125\n",
      "iteration 91, x1 4.0, x2 2.0010130405426025, loss -31.99999237060547, grad 0.0, grad 0.008104324340820312\n",
      "iteration 92, x1 4.0, x2 2.000931978225708, loss -31.99999237060547, grad 0.0, grad 0.0074558258056640625\n",
      "iteration 93, x1 4.0, x2 2.000857353210449, loss -31.999996185302734, grad 0.0, grad 0.00685882568359375\n",
      "iteration 94, x1 4.0, x2 2.000788688659668, loss -31.999998092651367, grad 0.0, grad 0.00630950927734375\n",
      "iteration 95, x1 4.0, x2 2.000725507736206, loss -32.0, grad 0.0, grad 0.0058040618896484375\n",
      "iteration 96, x1 4.0, x2 2.0006675720214844, loss -31.999998092651367, grad 0.0, grad 0.005340576171875\n",
      "iteration 97, x1 4.0, x2 2.0006141662597656, loss -31.999998092651367, grad 0.0, grad 0.004913330078125\n",
      "iteration 98, x1 4.0, x2 2.0005650520324707, loss -31.999998092651367, grad 0.0, grad 0.004520416259765625\n",
      "iteration 99, x1 4.0, x2 2.0005197525024414, loss -31.999998092651367, grad 0.0, grad 0.00415802001953125\n"
     ]
    }
   ],
   "source": [
    "def f(x1, x2):\n",
    "    return -8*x1 - 16*x2 + x1**2 + 4*x2**2\n",
    "\n",
    "x1 = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor(4, dtype=torch.float32, requires_grad=True)\n",
    "for i in range(100):\n",
    "    y = f(x1, x2)\n",
    "    y.backward()\n",
    "    print(f'iteration {i}, x1 {x1}, x2 {x2}, loss {y}, grad {x1.grad}, grad {x2.grad}')\n",
    "    with torch.no_grad():\n",
    "        x1 -= 0.01 * x1.grad\n",
    "        x2 -= 0.01 * x2.grad\n",
    "    x1.grad.zero_()\n",
    "    x2.grad.zero_()\n",
    "    #print(f'grad {x.grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:32:38.611808711Z",
     "start_time": "2023-10-06T08:32:38.533624946Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0, x1 tensor([4.], requires_grad=True), x2 tensor([2.4000], requires_grad=True), loss tensor([-16.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([16.])\n",
      "iteration 1, x1 tensor([4.], requires_grad=True), x2 tensor([2.0800], requires_grad=True), loss tensor([-31.3600], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([3.2000])\n",
      "iteration 2, x1 tensor([4.], requires_grad=True), x2 tensor([2.0160], requires_grad=True), loss tensor([-31.9744], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.6400])\n",
      "iteration 3, x1 tensor([4.], requires_grad=True), x2 tensor([2.0032], requires_grad=True), loss tensor([-31.9990], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.1280])\n",
      "iteration 4, x1 tensor([4.], requires_grad=True), x2 tensor([2.0006], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0256])\n",
      "iteration 5, x1 tensor([4.], requires_grad=True), x2 tensor([2.0001], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0051])\n",
      "iteration 6, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0010])\n",
      "iteration 7, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.0002])\n",
      "iteration 8, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([4.0054e-05])\n",
      "iteration 9, x1 tensor([4.], requires_grad=True), x2 tensor([2.0000], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([7.6294e-06])\n",
      "iteration 10, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.0000], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([1.9073e-06])\n",
      "iteration 11, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 12, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 13, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 14, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 15, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 16, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 17, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 18, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 19, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 20, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 21, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 22, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 23, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 24, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 25, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 26, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 27, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 28, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 29, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 30, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 31, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 32, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 33, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 34, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 35, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 36, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 37, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 38, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 39, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 40, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 41, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 42, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 43, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 44, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 45, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 46, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 47, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 48, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 49, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 50, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 51, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 52, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 53, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 54, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 55, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 56, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 57, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 58, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 59, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 60, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 61, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 62, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 63, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 64, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 65, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 66, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 67, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 68, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 69, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 70, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 71, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 72, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 73, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 74, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 75, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 76, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 77, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 78, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 79, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 80, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 81, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 82, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 83, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 84, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 85, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 86, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 87, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 88, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 89, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 90, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 91, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 92, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 93, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 94, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 95, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 96, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 97, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 98, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n",
      "iteration 99, x1 tensor([4.], requires_grad=True), x2 tensor([2.], requires_grad=True), loss tensor([-32.], grad_fn=<AddBackward0>), grad tensor([0.]), grad tensor([0.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "def f(x1, x2):\n",
    "    return -8*x1 - 16*x2 + x1**2 + 4*x2**2\n",
    "\n",
    "x1 = torch.tensor([4], dtype=torch.float32, requires_grad=True)\n",
    "x2 = torch.tensor([4], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "optimizer = torch.optim.SGD([x1, x2], lr=0.1)\n",
    "\n",
    "\n",
    "for i in range(100):\n",
    "    y = f(x1, x2)\n",
    "    optimizer.zero_grad()\n",
    "    y.backward()\n",
    "    optimizer.step()\n",
    "    print(f'iteration {i}, x1 {x1}, x2 {x2}, loss {y}, grad {x1.grad}, grad {x2.grad}')\n",
    "    #     with torch.no_grad():\n",
    "    #         x -= 0.1 * x.grad\n",
    "    #x.grad.zero_()\n",
    "    #print(f'grad {x.grad}')"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-10-06T08:35:44.388152044Z",
     "start_time": "2023-10-06T08:35:44.097189501Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
